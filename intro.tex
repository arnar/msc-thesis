Languages are among the most important and ubiquitous concepts in Computer Science.
In almost every sub-field of Computer Science, one can find specific languages for
describing and reasoning about the concepts of that field. Programming languages
are the best known example of course, but file formats, network protocols,
instruction sets and even the various diagrammatic techniques used in Software
Engineering can be considered as (visual) languages. Theoretical Computer Science has
specification languages and various logics. Natural Language Processing has markup
languages for describing voicing and sentence structure. Artificial Intelligence
uses specific languages for describing behaviour, the rules of games and the
constraints of planning problems. Indeed, it seems to be common practice in 
Computer Science to invent formalisms for specific problems, and these formalisms
very often involve some kinds of languages.

Any language consists of two parts: its \emph{syntax} and its \emph{semantics}.
The syntax defines what strings of symbols are valid, i.e. part of the language,
while the semantics defines the actual \emph{meaning} of any valid string.
Formal specification of syntax is very common, even in non-academic use of 
Computer Science. However, one can argue that what really 
defines the true nature
of a a language
is its semantics. Different languages are much rather set apart by different
semantics than different syntax.
This thesis is a study, by way of example, of one specific technique of specifying
semantics formally, namely \emph{Structural Operational Semantics}. To put things
in context, we'll start by an informal overview of this field.


\section{Structural Operational Semantics}

Structural Operational Semantics~\cite{Plotkin04a, Plotkin04b},
SOS or simply Operational Semantics
\footnote{The term Operational Semantics is sometimes used as a synonym for
Structural Operational Semantics (\emph{small-step} semantics) and Natural
Semantics (\emph{large-step} semantics). In this thesis we are mostly
concerned with the former.}
for short,
is a way of defining the meaning of terms in formal languages. By
\emph{formal languages} we mean any language for specifying ideas formally in the mathematical
sense. This includes programming languages, process languages for modelling and
verification as well as many others. By \emph{terms} we usually mean programs
or specifications written in these languages.

As the name indicates, SOS describes semantics in terms of program \emph{structure}
and the \emph{operations} a program carries out as it computes. 
An SOS specification of the semantics for a certain
language is a collection of rules. These rules specify how a term with a certain
structure behaves, by describing the operation that the next step of execution
of this term performs
(often on a hypothetical machine), and what is the term that should be executed
for the next step. An example of an SOS rule is
\begin{equation}\label{eq:intro_rule1}
    \frac{t_1 \trans{a} t_1' \quad t_2 \trans{b} t_2'}{\textsf{op(}t_1,t_2\textsf{)}\trans{b} t_2'}
\end{equation}
This rule specifies how a term of the form $\textsf{op(}t_1,t_2\textsf{)}$ behaves, where $t_1$
and $t_2$ can be any sub-terms as allowed by the syntax. This is seen by looking
at the left-hand side of the \emph{conclusion}, the part appearing below the line. 
The two expressions above
the line are called \emph{premises}. These are operations of the sub-terms
that describe when the rule is applicable to derive a step of computation
of the composite program $\textsf{op(}t_1,t_2\textsf{)}$.
This particular rule only applies if the operations in the premises can be
deduced from the collection of rules. Naturally, this depends in each case on what
the terms $t_1$ and $t_2$ actually are. If the operations in all the premises
are valid for given sub-terms, the label of the arrow and the right-hand side below the line
specify what is the operation performed by executing $\textsf{op(}t_1,t_2\textsf{)}$
and what is the term that results after doing so. As one can see, both of these may
be parameterised with information from the premises.

The hypothetical execution of terms proceeds by finding a rule that matches the
term to be executed and whose premises are met. This rule then specifies an operation,
i.e. \emph{a single step} of execution, and the term to use for finding the
next step. This process is repeated to create a sequence of operations.
It is important to note that execution in this context does not necessarily mean
execution on a real machine, but rather it is a useful abstract metaphor for reasoning about
program behaviour. We say the sequences of operations are steps in the execution
of a program on a hypothetical machine.

Often it is useful to indicate when such a sequence may stop, i.e. when the
program terminates. We often do this by designating a specific set of terms as
\emph{terminal}, meaning that when a sequence of operations reaches such a term,
the application of rules stops.
Sometimes this is the empty term, e.g. a program of the form
\textsf{print "Hello"; print "World"} might perform the following sequence of
operations
\[
    \textsf{print "Hello"; print "World"}
    \;\trans{!"Hello"}\;
    \textsf{print "World"}
    \;\trans{!"World"}\;
    \epsilon
\]
where the operation $!string$ stands for writing $string$ to the screen, and
$\epsilon$ is the empty program. In other settings the terminal terms may represent
values. For example, a functional programming language might specify the meaning
of the term $50 - 4 \times 2$ with the following sequence of operations.
\[
    50 - 4 \times 2 \;\trans{}\; 50 - 8 \;\trans{}\; 42
\]
In this case, the term $42$ is terminal since it contains no operators.

There is an important difference between the two approaches; in the former case the natural meaning
of the program \textsf{print "Hello"; print "World"} is determined by the sequence
of operations that its execution goes through, while in the latter the meaning
of the program $50 - 4 \times 2$ is represented by the final value
that the sequence reaches. Which one we choose depends on the particular setting
in which we are using SOS.

For the latter interpretation, where the meaning of a term is taken to be the
final value reached by a sequence of operations, there is an important thing to
note about an SOS specification (collection of rules). In the general case, there
is nothing that prevents the specification to contain rules that allow us to
deduce \emph{multiple} sequences of operations. For example, consider a system
that contains the rule~\ref{eq:intro_rule1} above, but also contains the following
rule.
\begin{equation}\label{eq:intro_rule2}
    \frac{t_1 \trans{a} t_1' \quad t_2 \trans{b} t_2'}{\textsf{op(}t_1,t_2\textsf{)}\trans{a} t_1'}
\end{equation}
Presented with a term of the form $\textsf{op(}t_1,t_2\textsf{)}$, we can see
that both rules may apply (depending on $t_1$ and $t_2$). If they do, we have
a case of non-determinism where the term may either be executed according to
rule~\ref{eq:intro_rule1} or rule~\ref{eq:intro_rule2}. In fact, an operator
with this pair of rules is known as a \emph{choice operator}, i.e. the execution
of the term $\textsf{op(}t_1,t_2\textsf{)}$ may choose whether it behaves like
$t_1$ or like $t_2$.

It is not difficult to see that, when we take the meaning of a program to be its
final value, if such non-determinism exists in the SOS specification, this meaning
is not well defined. A term might give rise to multiple sequences of operations and
thus multiple final values. Thus, when this view of meaning is taken, which is
common when dealing with functional languages, we often make the requirement that
the language's specification given by the rules is \emph{deterministic}, 
i.e. for each term
there is at most one operation and subsequent term that can be deduced from the
collection of rules. Such collection of rules are the topic of Chapter~\ref{ch:formats}
of this thesis.

In the other setting, where meaning of a term is taken to be the sequence of
operations it gives rise to, non-determinism is generally allowed. This is for example
the case in Process Algebra, where the meaning of a term is simply taken to be
determined, in some formal sense, by
the set of all possible behaviours it may generate. Two terms might for instance be considered
equal if they generate the same set of operation sequences.

\vspace{1em}

Sometimes the terms of the language don't contain enough information themselves
to model execution. This is for example the case in programming languages that have
variables which are globally bound. To find the value of a program term $3 + x$,
one needs to know the value of $x$. In SOS specifications, this is solved by using
\emph{configurations} instead of terms in the rules. A configuration is a predefined
structure which models the state of the execution completely. In the case of languages
with variables, a common formulation is to represent the states as pairs of a
term (with the same meaning as described above) and a \emph{variable store}, written
$\langle t, \Theta \rangle$. The variable store is in turn a function from the
set of variables to actual values.\footnote{Or terms in the case of lazy languages.}
A typical rule in such a language might look like this.
\begin{equation}\label{eq:intro_rule3}
    \frac{}{\langle \textsf{x := }n, \Theta\rangle \trans{}
            \langle \epsilon, \Theta[x\mapsto n]\rangle} \qquad n\in \mathbb{N}
\end{equation}
Note that this rule has no premises, which means that it applies whenever the term
to be executed matches the left-hand side of the conclusion.
The rule specifies that the operation of executing an assignment term, e.g.
\textsf{x := 28}, under a store $\Theta$, results in a configuration with an
empty term and a store that is identical to $\Theta$ except for its value in $x$,
which is mapped to $28$ (this is the conventional meaning of the $[\cdot\mapsto\cdot]$ syntax).
Formally there is nothing special about using configurations instead of terms;
configurations can themselves be considered ``terms'' of an extended language.

Another interesting thing to note about rule~\ref{eq:intro_rule3} is that it is
in fact a \emph{rule schema}. In other words, it represents a countably infinite
number of rules, indexed by the natural number $n$. This is common when a part
of the syntax of the language comes from a large domain such as $\mathbb{N}$.

\vspace{1em}

This thesis consists of three main chapters, each of which is an independent paper.
While their topics are in essence not related to each other, they all make use of
operational semantics in a central manner. Although familiarity with SOS helps, the
informal introduction above should provide the reader with enough background to read
Chapter~\ref{ch:tmi}, which exemplifies a fairly complex use case of SOS. 
Chapters~\ref{ch:decomp} and especially~\ref{ch:formats} use semantics in a more
formal way; these chapters will each introduce the necessary preliminaries needed
for their discussion. The following section introduces each chapter and highlights
their ties to operational semantics.


\section{Thesis contributions}

Over the course of 12 months, rather than working solely on one single MSc study project,
I have participated in several research projects at
Reykjavik University and at the Technical University of Eindhoven. The result of this work
are research contributions made by me and my co-authors to a few different fields
of Computer Science. Each of these projects have built on the theory of SOS; in fact
one of the projects (Chapter~\ref{ch:formats}) is only about the theory of SOS rule
systems, independent of their use.

The papers are arranged in order of increasing abstraction. Chapter~\ref{ch:tmi} uses SOS
to specify the semantics of an authorisation framework in a functional programming language.
The SOS specification presented is non-trivial, but is intended solely for clarifying
the semantics of this particular framework. The specification is accompanied by a
detailed discussion of the semantics as well as an implementation of the framework
in question.

Chapter~\ref{ch:decomp} is in the field of Process Algebra. It 
uses SOS to provide quotienting techniques a la~\cite{Larsen91} for extensions
to the process specification language CCS and Hennessy-Milner logic. CCS has a simple
operational semantics and the paper proves, using the semantics, a powerful theorem
for studying properties that include past modalities in a decompositional manner.

Finally, Chapter~\ref{ch:formats} goes one abstraction level above SOS specifications
and provides so-called \emph{meta-theorems} about rule systems that guarantee their
determinism and the idempotence of certain operators. The meta-theorems consist of
syntactic conditions on the rules themselves, such conditions are generally referred
to as \emph{SOS rule formats}.

In order to give the reader enough background for each chapter, we will now describe
the general field of each paper, its contributions as well as highlight the specific
contributions I made to each.


\subsection{Semantics of Transactional Memory Introspection}

Chapter~\ref{ch:tmi} builds on previous work of \cite{tmi}. In that paper we present
an authorisation architecture called Transactional Memory Introspection, or TMI.
The motivation for this architecture comes from the fact that Software Transactional
Memory has recently become a popular way of avoiding race conditions in concurrent
programs. Software Transactional Memory, or STM for short, tackles the issue of
shared memory by replacing programmer managed locks with transactions. Where programmers
would conventionally manage access to shared resources by careful lock placement,
they may use STM instead to offload this responsibility to a machine controlled
framework.

When using STM, programmers do away with lock management and instead mark sections
of code as \emph{atomic}. At run-time, an STM system will, as part of the program
in question, ensure that the accesses a single thread makes to shared resources
inside such atomic sections, appear atomic to other threads and moreover, are
completely isolated from the effects concurrent threads may have on this shared
state. Semantically this is equivalent to enforcing a rule which says that only
one thread may be running in an atomic section at each time.

The beauty of STM comes from the fact that the actual implementation 
does not enforce such strict policies, 
as that would hurt performance. Instead, multiple threads are allowed to
execute simultaneously inside atomic sections. Meanwhile, the STM system will carefully
monitor the actions of each one of the threads. Generally, the threads will be
accessing disjoint sets of resources, so most of the time this simultaneous execution
poses no problems. However, in the cases where threads in atomic sections do conflict
in their accesses, the STM system will notice and simply roll back some or all
of the threads involved, and restart their execution at the start of their atomic
sections. A rollback consists of undoing all work done by the threads, and will
be triggered in cases when the execution of a thread has violated the isolation
guarantee of the STM system. In practice, such violations happen in the minority
of cases, so often the overhead of this approach will be paid for by the overhead
saved in not using fine grained locking.

To implement the above, an STM system generally must provide
\begin{itemize}
    \item isolation of concurrent threads in atomic sections,
    \item monitoring of resource access to detect conflicts,
    \item the ability to abort and rollback execution of an atomic section.
\end{itemize}
In~\cite{tmi} we argue that these mechanisms can be very beneficial to the problem
of \emph{policy enforcement}. Policy enforcement (or authorisation) is required
in programs that handle sensitive data, to ensure that no illegal operations are
performed, such as releasing confidential data or otherwise violate the applications
policy. Traditionally this is done by careful code scrutiny and great effort on
the programming side. Just as with locking, this practice is prone to errors.

Since programs that use STM systems for synchronisation purposes are already paying
the price of monitoring and maintaining the ability to abort code, we conjectured
that these mechanisms could be used to simplify policy enforcement at a relatively
little extra cost. We identified three common problems (or errors) in modern policy
enforcement code.
\begin{itemize}
    \item \emph{Time of check to time of use} (TOCTTOU) bugs. These happen when
    a policy decision is made prior to access, but the state used for the decisions
    is mutated in between by a concurrent thread.
    \item Difficulty in guaranteeing \emph{complete mediation}, i.e. ensuring that
    any access, explicit or implicit, is accompanied by the relevant policy check.
    This is non-trivial in complex systems and empirical studies show that this
    is a source of several security holes in critical software.
    \item Difficulty in dealing with authorisation errors, when a policy violation
    has been detected, the system state must be carefully reset in order not to leak
    sensitive information or implicitly cause other policy violations.
\end{itemize}
The first of these is simply a synchronisation issue, and could be solved with locking.
However, STM systems provide a synchronisation mechanism with added benefits; we
can make use of its careful monitoring and abort capabilities to severely reduce
the second and third difficulties.

When an application that makes use of TMI (which implies the use of STM) runs,
any accesses made inside atomic sections are inspected by the STM system. TMI
hooks into this inspection and also notifies an application specific security
manager, which checks if the access is allowed by the application policy. At any
time, the security manager has the capability to veto an access due to policy
violation, in which case the abort mechanism of the STM is invoked. In one fell
swoop this solves the issue of complete mediation, since the STM diligently
inspects every access, as well as the issue of error handling since the rollback
puts the system back into a consistent state and the isolation guarantees of the
STM make sure that no concurrent thread gained knowledge of the actions leading
up to the policy violation.

Our previous work of~\cite{tmi} consists of an extended discussion of the above,
accompanied by a proof-of-concept implementation based on a prototype STM framework
for Java~\cite{hlm06}. However, while working with TMI and STM systems in general,
we discovered that there are a great number of subtleties in the behaviour of
unusual edge cases. An informal discussion, and even an implementation, did not
provide a thorough understanding of the semantics of TMI. Thus the contribution
presented in Chapter~\ref{ch:tmi} consists of the formal specification of the semantics
of our architecture, in the form of an extension to the semantics of the Haskell
STM system~\cite{haskellstm}. The semantics is accompanied by a matching implementation.

My specific contributions to Chapter~\ref{ch:tmi} consist of most of the technical
work involved. I built the extension of the Haskell STM semantics, which went
through several iterations of discussions with my co-author and revisions. In
parallel I wrote the implementation in Haskell, which provided a lot of insight
into the design decisions behind the semantic specification. I wrote the initial
versions of most of the text, except for the introduction and the background on
STM and TMI. All sections underwent a rewriting phase carried out jointly by
me and my co-author.

This work has been submitted to the ACM SIGPLAN Fourth Workshop on Programming
Languages and Analysis for Security (PLAS 2009), scheduled for June 15th 2009
in Dublin, Ireland. The author notification will arrive before the final version
of this thesis is prepared. The submission is identical to the version presented
here.