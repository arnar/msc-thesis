Languages are among the most important and ubiquitous concepts in Computer Science.
In almost every sub-field of Computer Science, one can find specific languages for
describing and reasoning about the concepts of that field. Programming languages
are the best known example of course, but file formats, network protocols,
instruction sets and even the various diagrammatic techniques used in Software
Engineering can be considered as (visual) languages. Theoretical Computer Science has
specification languages and various logics. Natural Language Processing has markup
languages for describing voicing and sentence structure. Artificial Intelligence
uses specific languages for describing behaviour, the rules of games and the
constraints of planning problems. Indeed, it seems to be common practice in 
Computer Science to invent formalisms for specific problems, and these formalisms
very often involve some kinds of languages.

Any language consists of two parts: its \emph{syntax} and its \emph{semantics}.
The syntax defines what strings of symbols are valid, i.e. part of the language,
while the semantics defines the actual \emph{meaning} of any valid string.
Formal specification of syntax is very common, even in non-academic use of 
Computer Science. However, one can argue that what really 
defines the true nature
of a a language
is its semantics. Different languages are much rather set apart by different
semantics than different syntax.
This thesis is a study, by way of example, of one specific technique of specifying
semantics formally, namely \emph{Structural Operational Semantics}. To put things
in context, we'll start by an informal overview of this field.


\section{Structural Operational Semantics} % (fold)
\label{sec:intro_sos}

Structural Operational Semantics~\cite{Plotkin04a, Plotkin04b},
SOS or simply Operational Semantics
\footnote{The term Operational Semantics is sometimes used as a synonym for
Structural Operational Semantics (\emph{small-step} semantics) and Natural
Semantics (\emph{large-step} semantics). In this thesis we are mostly
concerned with the former.}
for short,
is a way of defining the meaning of terms in formal languages. By
\emph{formal languages} we mean any language for specifying ideas formally in the mathematical
sense. This includes programming languages, process languages for modelling and
verification as well as many others. By \emph{terms} we usually mean programs
or specifications written in these languages.

As the name indicates, SOS describes semantics in terms of program \emph{structure}
and the \emph{operations} a program carries out as it computes. 
An SOS specification of the semantics for a certain
language is a collection of rules. These rules specify how a term with a certain
structure behaves, by describing the operation that the next step of execution
of this term performs
(often on a hypothetical machine), and what is the term that should be executed
for the next step after that. An example of an SOS rule is
\begin{equation}\label{eq:intro_rule1}
    \frac{t_1 \trans{a} t_1' \quad t_2 \trans{b} t_2'}{\textsf{op(}t_1,t_2\textsf{)}\trans{b} t_2'}
\end{equation}
This rule specifies how a term of the form $\textsf{op(}t_1,t_2\textsf{)}$ behaves, where $t_1$
and $t_2$ can be any sub-terms as allowed by the syntax. This is seen by looking
at the left-hand side of the \emph{conclusion}, the part appearing below the line. 
The two expressions above
the line are called \emph{premises}. These are operations of the sub-terms
that describe when the rule is applicable to derive a step of computation
of the composite program $\textsf{op(}t_1,t_2\textsf{)}$.
This particular rule only applies if the operations in the premises can be
deduced from the collection of rules. Naturally, this depends in each case on what
the terms $t_1$ and $t_2$ actually are. If the operations in all the premises
are valid for given sub-terms, the label of the arrow and the right-hand side below the line
specify what is the operation performed by executing $\textsf{op(}t_1,t_2\textsf{)}$
and what is the term that results after doing so. As one can see, both of these may
be parameterised with information from the premises.

The hypothetical execution of terms proceeds by finding a rule that matches the
term to be executed and whose premises are met. This rule then specifies an operation,
i.e. \emph{a single step} of execution, and the term to use for finding the
next step. This process is repeated to create a sequence of operations.
It is important to note that execution in this context does not necessarily mean
execution on a real machine, but rather it is a useful abstract metaphor for reasoning about
program behaviour. We say the sequences of operations are steps in the execution
of a program on a hypothetical machine.

Often it is useful to indicate when such a sequence may stop, i.e. when the
program terminates. We often do this by designating a specific set of terms as
\emph{terminal}, meaning that when a sequence of operations reaches such a term,
the application of rules stops.
Sometimes this is the empty term, e.g. a program of the form
\textsf{print "Hello"; print "World"} might perform the following sequence of
operations
\[
    \textsf{print "Hello"; print "World"}
    \;\trans{!"Hello"}\;
    \textsf{print "World"}
    \;\trans{!"World"}\;
    \epsilon
\]
where the operation $!string$ stands for writing $string$ to the screen, and
$\epsilon$ is the empty program. In other settings the terminal terms may represent
values. For example, a functional programming language might specify the meaning
of the term $50 - 4 \times 2$ with the following sequence of operations.
\[
    50 - 4 \times 2 \;\trans{}\; 50 - 8 \;\trans{}\; 42
\]
In this case, the term $42$ is terminal since it contains no operators.

There is an important difference between the two approaches; in the former case the natural meaning
of the program \textsf{print "Hello"; print "World"} is determined by the sequence
of operations that its execution goes through, while in the latter the meaning
of the program $50 - 4 \times 2$ is represented by the final value
that the sequence reaches. Which one we choose depends on the particular setting
in which we are using SOS.

For the latter interpretation, where the meaning of a term is taken to be the
final value reached by a sequence of operations, there is an important thing to
note about an SOS specification (collection of rules). In the general case, there
is nothing that prevents the specification to contain rules that allow us to
deduce \emph{multiple} sequences of operations. For example, consider a system
that contains the rule~\ref{eq:intro_rule1} above, but also contains the following
rule.
\begin{equation}\label{eq:intro_rule2}
    \frac{t_1 \trans{a} t_1' \quad t_2 \trans{b} t_2'}{\textsf{op(}t_1,t_2\textsf{)}\trans{a} t_1'}
\end{equation}
Presented with a term of the form $\textsf{op(}t_1,t_2\textsf{)}$, we can see
that both rules may apply (depending on $t_1$ and $t_2$). If they do, we have
a case of non-determinism where the term may either be executed according to
rule~\ref{eq:intro_rule1} or rule~\ref{eq:intro_rule2}. In fact, an operator
with this pair of rules is known as a \emph{choice operator}, i.e. the execution
of the term $\textsf{op(}t_1,t_2\textsf{)}$ may choose whether it behaves like
$t_1$ or like $t_2$.

It is not difficult to see that, when we take the meaning of a program to be its
final value, if such non-determinism exists in the SOS specification, this meaning
is not well defined. A term might give rise to multiple sequences of operations and
thus multiple final values. Thus, when this view of meaning is taken, which is
common when dealing with functional languages, we often make the requirement that
the language's specification given by the rules is \emph{deterministic}, 
i.e. for each term
there is at most one operation and subsequent term that can be deduced from the
collection of rules. Such collection of rules are the topic of Chapter~\ref{ch:formats}
of this thesis.

In the other setting, where meaning of a term is taken to be the sequence of
operations it gives rise to, non-determinism is generally allowed. This is for example
the case in Process Algebra, where the meaning of a term is simply
determined, in some formal sense, by
the set of all possible behaviours it may generate. Two terms might for instance be considered
equal if they generate the same set of operation sequences.

\vspace{1em}

Sometimes the terms of the language don't contain enough information themselves
to model execution. This is for example the case in programming languages that have
variables which are globally bound. To find the value of a program term $3 + x$,
one needs to know the value of $x$. In SOS specifications, this is solved by using
\emph{configurations} instead of terms in the rules. A configuration is a predefined
structure which models the state of the execution completely. In the case of languages
with variables, a common formulation is to represent the states as pairs of a
term (with the same meaning as described above) and a \emph{variable store}, written
$\langle t, \Theta \rangle$. The variable store is in turn a function from the
set of variables to actual values.\footnote{Or terms in the case of lazy languages.}
A typical rule in such a language might look like this.
\begin{equation}\label{eq:intro_rule3}
    \frac{}{\langle \textsf{x := }n, \Theta\rangle \trans{}
            \langle \epsilon, \Theta[x\mapsto n]\rangle} \qquad n\in \mathbb{N}
\end{equation}
Note that this rule has no premises, which means that it applies whenever the term
to be executed matches the left-hand side of the conclusion.
The rule specifies that the operation of executing an assignment term, e.g.
\textsf{x := 28}, under a store $\Theta$, results in a configuration with an
empty term and a store that is identical to $\Theta$ except for its value in $x$,
which is mapped to $28$ (this is the conventional meaning of the $[\cdot\mapsto\cdot]$ syntax).
Formally there is nothing special about using configurations instead of terms;
configurations can themselves be considered ``terms'' of an extended language.

Another interesting thing to note about rule~\ref{eq:intro_rule3} is that it is
in fact a \emph{rule schema}. In other words, it represents a countably infinite
number of rules, indexed by the natural number $n$. This is common when a part
of the syntax of the language comes from a large domain such as $\mathbb{N}$.

\vspace{1em}

This thesis consists of three main chapters, each of which is an independent paper.
While their topics are in essence not related to each other, they all make use of
operational semantics in a central manner. Although familiarity with SOS helps, the
informal introduction above should provide the reader with enough background to read
Chapter~\ref{ch:tmi}, which exemplifies a fairly complex use case of SOS. 
Chapters~\ref{ch:decomp} and especially Chapter~\ref{ch:formats} use semantics in a more
formal way; these chapters will each introduce the necessary preliminaries needed
for their discussion. The following section introduces each chapter and highlights
their ties to operational semantics.

% (end)

\section{Thesis contributions} % (fold)

Over the course of 12 months, rather than working solely on one single MSc study project,
I have participated in several research projects at
Reykjavik University and at the Technical University of Eindhoven. The result of this work
are research contributions made by my co-authors and me to a few different fields
of Computer Science. Each of these projects have built on the theory of SOS; in fact
one of the projects (Chapter~\ref{ch:formats}) is only about the theory of SOS rule
systems, independent of their use.

The papers are arranged in order of increasing abstraction. Chapter~\ref{ch:tmi} uses SOS
to specify the semantics of an authorisation framework in a functional programming language.
The SOS specification presented is non-trivial, but is intended solely for clarifying
the semantics of this particular framework. The specification is accompanied by a
detailed discussion of the semantics as well as an implementation of the framework
in question.

Chapter~\ref{ch:decomp} is in the field of Process Algebra. It 
uses SOS to provide quotienting techniques a la~\cite{Larsen91} for extensions
to the process specification language CCS and Hennessy-Milner logic. CCS has a simple
operational semantics and the paper proves, using the semantics, a powerful theorem
for studying properties, that include past modalities, in a decompositional manner.

Finally, Chapter~\ref{ch:formats} goes one abstraction level above SOS specifications
and provides so-called \emph{meta-theorems} about rule systems that guarantee their
determinism and the idempotence of certain operators. The meta-theorems consist of
syntactic conditions on the rules themselves, such conditions are generally referred
to as \emph{SOS rule formats}.

In order to give the reader enough background for each chapter, we will now describe
the general field of each paper, its contributions as well as highlight the specific
contributions I made to each.
% (end)

\subsection{Semantics of Transactional Memory Introspection} % (fold)

Chapter~\ref{ch:tmi} builds on previous work of \cite{tmi}. In that paper we present
an authorisation architecture called Transactional Memory Introspection, or TMI.
The motivation for this architecture comes from the fact that Software Transactional
Memory has recently become a popular way of avoiding race conditions in concurrent
programs. Software Transactional Memory, or STM for short, tackles the issue of
shared memory by replacing programmer managed locks with transactions. Where programmers
would conventionally manage access to shared resources by careful lock placement,
they may use STM instead to offload this responsibility to a machine controlled
framework.

When using STM, programmers do away with lock management and instead mark sections
of code as \emph{atomic}. At run-time, an STM system will, as part of the program
in question, ensure that the accesses a single thread makes to shared resources
inside such atomic sections, appear atomic to other threads and moreover, are
completely isolated from the effects concurrent threads may have on this shared
state. Semantically this is equivalent to enforcing a rule which says that only
one thread may be running in an atomic section at each time.

The beauty of STM comes from the fact that the actual implementation 
does not enforce such strict policies, 
as that would hurt performance. Instead, multiple threads are allowed to
execute simultaneously inside atomic sections. Meanwhile, the STM system will carefully
monitor the actions of each one of the threads. Generally, the threads will be
accessing disjoint sets of resources, so most of the time this simultaneous execution
poses no problems. However, in the cases where threads in atomic sections do conflict
in their accesses, the STM system will notice and simply roll back some or all
of the threads involved, and restart their execution at the start of their atomic
sections. A rollback consists of undoing all work done by the threads, and will
be triggered in cases when the execution of a thread has violated the isolation
guarantee of the STM system. In practice, such violations happen in the minority
of cases, so often the overhead of this approach will be paid for by the overhead
saved in not using fine grained locking.

To implement the above, an STM system generally must provide
\begin{itemize}
    \item isolation of concurrent threads in atomic sections,
    \item monitoring of resource access to detect conflicts,
    \item the ability to abort and rollback execution of an atomic section.
\end{itemize}
In~\cite{tmi} we argue that these mechanisms can be very beneficial to the problem
of \emph{policy enforcement}. Policy enforcement (or authorisation) is required
in programs that handle sensitive data, to ensure that no illegal operations are
performed, such as releasing confidential data or otherwise violate the applications
policy. Traditionally this is done by careful code scrutiny and great effort on
the programming side. Just as with locking, this practice is prone to errors.

Since programs that use STM systems for synchronisation purposes are already paying
the price of monitoring and maintaining the ability to abort code, we conjectured
that these mechanisms could be used to simplify policy enforcement at a relatively
little extra cost. We identified three common problems (or errors) in modern policy
enforcement code.
\begin{itemize}
    \item \emph{Time of check to time of use} (TOCTTOU) bugs. These happen when
    a policy decision is made prior to access, but the state used for the decisions
    is mutated in between by a concurrent thread.
    \item Difficulty in guaranteeing \emph{complete mediation}, i.e. ensuring that
    any access, explicit or implicit, is accompanied by the relevant policy check.
    This is non-trivial in complex systems and empirical studies show that this
    is a source of several security holes in critical software.
    \item Difficulty in dealing with authorisation errors, when a policy violation
    has been detected, the system state must be carefully reset in order not to leak
    sensitive information or implicitly cause other policy violations.
\end{itemize}
The first of these is simply a synchronisation issue, and could be solved with locking.
However, STM systems provide a synchronisation mechanism with added benefits; we
can make use of its careful monitoring and abort capabilities to severely reduce
the second and third difficulties.

When an application that makes use of TMI (which implies the use of STM) runs,
any accesses made inside atomic sections are inspected by the STM system. TMI
hooks into this inspection and also notifies an application specific security
manager, which checks if the access is allowed by the application policy. At any
time, the security manager has the capability to veto an access due to policy
violation, in which case the abort mechanism of the STM is invoked. In one fell
swoop this solves the issue of complete mediation, since the STM diligently
inspects every access, as well as the issue of error handling since the rollback
puts the system back into a consistent state and the isolation guarantees of the
STM make sure that no concurrent thread gained knowledge of the actions leading
up to the policy violation.

Our previous work of~\cite{tmi} consists of an extended discussion of the above,
accompanied by a proof-of-concept implementation based on a prototype STM framework
for Java~\cite{hlm06}. However, while working with TMI and STM systems in general,
we discovered that there are a great number of subtleties in the behaviour of
unusual edge cases. An informal discussion, and even an implementation, did not
provide a thorough understanding of the semantics of TMI. Thus the contribution
presented in Chapter~\ref{ch:tmi} consists of the formal specification of the semantics
of our architecture, in the form of an extension to the semantics of the Haskell
STM system~\cite{haskellstm}. The semantics is accompanied by a matching implementation.

My specific contributions to Chapter~\ref{ch:tmi} consist of most of the technical
work involved. I built the extension of the Haskell STM semantics, which went
through several iterations of discussions with my co-author and revisions. In
parallel I wrote the implementation in Haskell, which provided a lot of insight
into the design decisions behind the semantic specification. I wrote the initial
versions of most of the text, except for the introduction and the background on
STM and TMI. All sections underwent a rewriting phase carried out jointly by
my co-author and me.

This work has been submitted to the ACM SIGPLAN Fourth Workshop on Programming
Languages and Analysis for Security (PLAS 2009), scheduled for June 15th 2009
in Dublin, Ireland. The author notification will arrive before the final version
of this thesis is prepared. The submission is identical to the version presented
here.

% (end)

\subsection{Decompositional Reasoning about the History of Parallel Processes} % (fold)

In \emph{model checking}, process specification languages are used to construct
behavioural models of software and hardware systems, in particular reactive
systems. These models are then used
for detailed analysis of a system's behaviour. The process specification languages
are usually accompanied by logic languages that allow the designer to specify an
array of desirable, or non-desirable, properties. Such properties include 
\begin{itemize}
    \item \emph{liveness properties,} guaranteeing that the system can always 
    continue no matter the input it faces;
    \item \emph{safety properties} such that the system will never perform certain critical
    operations unless it is in a state where it is safe to do so; and
    \item \emph{security properties} such as ensuring operations are properly authorised.
\end{itemize}

In most cases, the semantics of the specification languages is given as Structural
Operational Semantics. A model is simply a term of the language, where the possible
execution paths given by the semantics define the behavioural capabilities of
the model. The properties to be checked are then described as logical expressions,
often in logics which include modal operators (e.g. the model \emph{can} perform
a particular operation) or operators on the possible sequences of operations (such
as \emph{on each possible path there is a term with a particular property}).
Examples of the former include Hennessy-Milner logic~\cite{Hennessy85} 
and of the latter CTL~\cite{CE81} and LTL~\cite{Pnueli77}.

Most specification languages, or \emph{process calculi}, 
include an operator that allows one to describe a process resulting from the 
parallel composition of two or more agents
Usually this models interleaving
concurrency, but in general a model of a process, which is composed of multiple
parallel components, can exhibit the behaviour of any of the components in addition
to certain synchronising behaviour where two or more of the components communicate.
This means that the number of possible behaviours of a system grows exponentially
in the number of parallel components.

Model checking is the act of testing if a certain model satisfies a property
described by a logical expression. The resource usage of the algorithms that perform
model checking depends directly on the number of possible behaviours that the
model can exhibit, as the algorithms must exhaustively check every possible
behaviour. Since many systems are best described as a number of smaller systems
working in parallel such state space explosion is one of the biggest hurdles that
must be overcome to make model checking practically useful.

One way of addressing the problem of the great increase in possible behaviours
when systems are composed in parallel, is \emph{decompositional reasoning}. Say
we have a system made of the components $P$ and $Q$ composed in parallel, 
usually written as
\[
    \textsf{System} = P \parallel Q.
\]
We want to answer the following question: 
\emph{Does \textsf{System} satisfies a given property described by a
logic formula $\phi$?} 
The number of behaviours we may need to
examine can potentially be the product of the number of behaviours possible of $P$ and $Q$,
and even if $P$ and $Q$ alone are of moderate size, this product can be unmanageably large.
However, in many cases we can use our knowledge of the system $Q$ to construct
another property (i.e. a logic formula) $\psi$ such that the following question
of $P$
is equivalent to our original one of \textsf{System}:
\emph{Does $P$ satisfy the property $\psi$?} In other words, if we can prove the
bi-implication
\[
    P\parallel Q \textrm{ satisfies } \phi \quad\Leftrightarrow\quad
    P \textrm{ satisfies } \psi,
\]
we have reduced the size of the model checking problem to the size of $P$.
%
However, the property $\psi$ is constructed from $\phi$ \emph{and} the system $Q$.
The property $\psi$ may thus be more complex than $\phi$, and the size of $Q$ has
a direct impact on the difference. However this kind of reasoning has been shown
to be efficient in model checking composed systems~\cite{Andersen95,LaroussinieL98}.

Another use of decompositional reasoning arises when coupling it with synthesis 
of models from logical specifications. Suppose that a logical formula $\phi$ gives 
the specification of the expected behaviour of the system to be built and that 
our system has the form $P \parallel Q$. Assume furthermore that we have been given 
component $Q$ off the shelf. A natural question to ask is whether we can we build 
$P$ so that the resulting system will satisfy the specification $\phi$. Using 
quotienting, this question can be reduced to whether we can build a model of the
formula $\psi$. Such models can be constructed using known model construction 
techniques for many logics of interest. 

Decompositional reasoning in essence tries to describe global properties of
composed systems in terms of the local properties of its components, 
and dates back to the work of Larsen and Xinxin~\cite{Larsen91}.
It has been further developed by several studies~\cite{Giannakopoulou05,Xie05,Andersen95,LaroussinieL95}.
Chapter~\ref{ch:decomp} of this thesis aims to extend and apply such techniques
in a setting which, to the best of my knowledge, has not been attempted before,
namely the setting where the models maintain a record of their execution history.

\vspace{1em}

In process calculus, one generally talks about \emph{states} of execution. In the
SOS sense, these states are the terms (or configurations) of the transition system
generated by the semantic rules for the process calculus, starting from a designated
initial state. The initial state is the term that represents the behavioural model
of the system being described. Just as the initial state encodes the behavioural
capabilities of the system, an intermediate state reached after performing some
operations usually represents the behavioural capabilities of the system at that point.
In particular, the intermediate states usually do not contain any information about
the \emph{past}, i.e. the operations performed by the system before reaching said state.

For many properties that we want to check, this poses no problems. However,
by enriching intermediary states with information about the past behaviour of 
the system in reaching them, some kinds of properties become easier to reason
about. This includes for example epistemic properties, where one looks at the
knowledge gained by agents (partially) observing the system during execution
\cite{Mousavi07-LPAR}.

As for decompositional reasoning, the literature is rich with studies of process systems
that involve the past~\cite{HennessyS85,Phillips06,Laroussinie00,DeNicola:1990}.
Our contribution in Chapter~\ref{ch:decomp} is combining the two fields. We build
on a core subset of the process calculus CCS~\cite{Milner80} and extend
the formalism based on its operational semantics to states that maintain full
information about the execution history of processes. We similarly extend Hennessy--Milner
Logic~\cite{HennessyM80} with modalities that look back in the history, 
in contrast with the standard HML modalities that look forward to the possible
future behaviours.
The main theorem of the chapter
proves that decompositional reasoning can be applied to parallel processes in this
setting.

The results presented here are however only a milestone towards a richer theory
of decompositional reasoning about the past, albeit an important milestone. Work
is currently underway to extend these results further, to include the ability to
reason about recursive properties, which greatly enhances the expressiveness of
our extended logic.

The technical work and writing of Chapter~\ref{ch:decomp} is for the most part
mine apart from the introduction.

% section decompositional_reasoning_about_the_history_of_parallel_processes (end)

\subsection{Rule Formats for Determinism and Idempotency} % (fold)

When designing a language, or analysing the semantics of existing ones,
one is often interested in certain algebraic properties. For example, the language
might contain a operator $+$, which combines two subterms in some way. Naming
an operator $+$ immediately signals to the user of the language that this operator
possesses some properties of regular arithmetic addition, such as commutativity
and associativity. These are algebraic properties described by the equations
\[
    x + y = y + x  \quad\textrm{and}\quad (x + y) + z = x + (y + z).
\]
Such properties are often desirable to have, both to simplify the mental model
one has of a language and also for technical purposes. For example, associativity
can be very useful for automatically distributing large computations across an
array of processors.

The properties we are are interested in are many. Besides commutativity and
associativity, other useful properties include structural congruences, zero and
unit elements of operators, determinism and idempotency. In Chapter~\ref{ch:formats}
we are concerned with the last two; determinism of operators, which means that
at most one operation and subsequent term can be produced by the execution of
a given term; and idempotency of operators, which is best described by the algebraic
equation
\[
    \forall x : f(x,x) = x
\]
where $f$ is an operator of the language. As noted in Section~\ref{sec:intro_sos}
earlier in this introduction, determinism is often a very important property.
Idempotency is also a very natural requirement to make of certain operators.

Given an SOS specification of a language, such properties can be proven to hold.
Such proofs generally consists of structural induction on the syntax of the language
and/or on proof structures that arise when the SOS rules are used to deduce the
operations that a term can afford. Often one has to consider a number of cases,
and for real life languages, both the syntax and the number of rules can become
reasonably large, so that often such proofs are tedious listings of a great
number of cases. Such proofs are generally tedious to construct and check, as well
as prone to errors. Furthermore such proofs must be made in the context of one
language, and if the syntax or semantics of an evolving language change any existing
proofs must be adapted and re-checked.

This has given rise to the \emph{meta-theory of SOS}, in particular so-called
\emph{rule formats}~\cite{Aceto01,Mousavi07-TCS}. By putting constraints on the
SOS rules used for a language (or a part of a language), one can prove properties
such as those above, in general for any language which as an SOS specification that
meets the constraints. Often the constraints can be kept purely syntactical, in
which case it becomes a relatively easy matter to check a certain specification
against those constraints. Such constraints are called rule formats, and are
accompanied by meta-theorems that state that any semantics that meets the constraints
defines a language that has a particular property.

Many rule formats exist already. There are rule formats for 
commutativity \cite{Mousavi05-IPL} and 
associativity \cite{Mousavi08-CONCUR} of operators, and congruence of behavioral 
equivalences \cite{Verhoef95}, as well as for less algebraic properties such
as non-interference \cite{Tini04} and stochasticity \cite{Lanotte05}.
%
In Chapter~\ref{ch:formats} we present two related formats. 
One guarantees the determinism of a transition system (or a subset thereof) 
and the other guarantees idempotency of a given operator.

My contributions to this work include both technical developments and writing.
This work has been published in the 3rd International Conference on
Fundamentals of Software Engineering (FSEN'09) in April 2009. The three proofs
appearing in the chapter underwent peer-review through the FSEN program committee,
but were omitted from the conference publication due to space constraints. Otherwise
the published version is identical to the one that appears here.

% (end)

\section{Acknowledgements}

The past year has been extremely interesting for me and has convinced me to 
pursue a career in Computer Science research. I feel privileged for the opportunities
I have been granted to work on difficult and interesting problems, aside world-class
researchers in their respective fields.

My sincerest thanks go to my supervisor, Luca Aceto, for sharing his great
enthusiasm for computer science, and his extensive experience of research. 
He has always been available for me and willing to answer all of my questions,
no matter how many times I have asked them before.
Working with him has been a pleasure and an extremely valuable experience for which
I am very grateful.

I would also like to thank {\'U}lfar Erlingsson who, besides being a great teacher,
has also become a good friend of mine through our long and enjoyable conversations.
He has taken it upon himself to be my mentor in many ways, well beyond his duty, 
and from this I have benefited greatly.
I owe {\'U}lfar a great debt for actively singing my praises to many of the world-leading
experts, giving me a head-start in the research community.
It is now up to me to live up to them.

I thank MohammadReza Mousavi for generously hosting me for a visit
to TU/e during the fall of 2008. Mohammad's drive is something I aspire for
and his advice and confidence in me has helped me achieve things that I am
proud of.

I also want to thank Anna Ing{\'o}lfsd{\'o}ttir and Michel Reniers for their
support and enjoyable collaboration. I give special thanks to Maja Mei-Xin Aceto
for commenting on the late drafts of this thesis with a drawing of a cheerful 
guy with six legs.

Thanks to Stef{\'a}n Freyr, Hilmar and P{\'a}lmi for their friendship and for
tolerating my constant interruptions. Thanks to Gu\dh{}mundur Bjarni, for always
being there with his support, friendship and honest opinion.

My greatest thanks go to my dearest friend, Hanna Mar{\'i}a. Without your encouragement
and endless support, I would never have started this in the first place.

% subsubsection rule_formats_for_determinism_and_idempotency (end)